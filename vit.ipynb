{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dataset import cifar10\n",
    "from vision import util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cifar10.CIFAR10DataSet()\n",
    "downloader = dataset.train_dataloader()\n",
    "images, labels = dataset.get_batch(downloader)\n",
    "util.show_images(images, dataset.mean, dataset.std)\n",
    "print(' '.join('%5s' % dataset.get_classes()[l] for l in labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take one image and build the initial patch building and positional embedding step.\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of images is batch (B), channel (C), height (H), width (W)\n",
    "B, C, H, W = images.shape\n",
    "B, C, H, W"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1\n",
    "The standard transformer receives as input a $1D$ sequence of token embeddings. To handle $2D$ images, we reshape the image $x \\in\\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $x_p \\in\\mathbb{R}^{N \\cdot P^2 \\cdot C}$, where $(H,W)$ is the resolution of the original image, and $C$ is the number of channels, $(P,P)$ is the resolution of each image patch, and $N = \\frac{HW}{P^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = [torch.arange(0, 32) for _ in range(32)]\n",
    "x = torch.stack(x_t, dim=-1)\n",
    "kc, kh, kw = 2, 2, 2  # kernel size\n",
    "dc, dh, dw = 2, 2, 2  # stride\n",
    "patches = x.unfold(1, kc, dc).unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "unfold_shape = patches.size()\n",
    "patches = patches.contiguous().view(patches.size(0), -1, kc, kh, kw)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/patch-making-does-pytorch-have-anything-to-offer/33850/11\n",
    "x = torch.randn(1, 500, 500, 500)  # batch, c, h, w\n",
    "kc, kh, kw = 64, 64, 64  # kernel size\n",
    "dc, dh, dw = 64, 64, 64  # stride\n",
    "patches = x.unfold(1, kc, dc).unfold(2, kh, dh).unfold(3, kw, dw)\n",
    "unfold_shape = patches.size()\n",
    "patches = patches.contiguous().view(patches.size(0), -1, kc, kh, kw)\n",
    "print(patches.shape)\n",
    "\n",
    "# Reshape back\n",
    "patches_orig = patches.view(unfold_shape)\n",
    "output_c = unfold_shape[1] * unfold_shape[4]\n",
    "output_h = unfold_shape[2] * unfold_shape[5]\n",
    "output_w = unfold_shape[3] * unfold_shape[6]\n",
    "patches_orig = patches_orig.permute(0, 1, 4, 2, 5, 3, 6).contiguous()\n",
    "patches_orig = patches_orig.view(1, output_c, output_h, output_w)\n",
    "\n",
    "# Check for equality\n",
    "print((patches_orig == x[:, :output_c, :output_h, :output_w]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch embed layer that takes a 2D image to create embed patches of size P\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=96):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embd = nn.Conv2d(\n",
    "            in_chans,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size, \n",
    "            stride=patch_size\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f\"PatchEmbedding: x.shape: {x.shape}\")\n",
    "        x = self.patch_embd(x)\n",
    "        # print(f\"PatchEmbedding: patch_embd(x).shape: {x.shape}\")\n",
    "        x = x.flatten(2)\n",
    "        # print(f\"PatchEmbedding: flatten(patch_embed(x)).shape: {x.shape}\")\n",
    "        x = x.transpose(1,2)\n",
    "        # print(f\"PatchEmbedding: transpose(flatten(patch_embed(x)),(1,2)).shape: {x.shape}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(dataset, split, batch_size=32):\n",
    "    if split == 'train':\n",
    "        downloader = dataset.train_dataloader(batch_size=batch_size)\n",
    "    elif split == 'val':\n",
    "        downloader = dataset.val_dataloader(batch_size=batch_size)\n",
    "    elif split == 'test':\n",
    "        downloader = dataset.test_dataloader(batch_size=batch_size)\n",
    "    else:\n",
    "        raise AttributeError(f'Invalid Split parameter ({split}) provided.')\n",
    "    x, y_label = dataset.get_batch(downloader)\n",
    "    y = F.one_hot(y_label, num_classes=len(dataset.get_classes()))\n",
    "    return x,y.float()\n",
    "\n",
    "x, y = get_batch(dataset, \"train\")\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "x_val, y_val = get_batch(dataset, \"val\")\n",
    "print(x_val.shape, y_val.shape)\n",
    "\n",
    "x_test, y_test= get_batch(dataset, \"test\")\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.show_images(x, dataset.mean, dataset.std)\n",
    "print(' '.join('%5s' % dataset.get_classes()[l] for l in torch.argmax(y, dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "util.show_images(x_val, dataset.mean, dataset.std)\n",
    "print(' '.join('%5s' % dataset.get_classes()[l] for l in torch.argmax(y_val, dim=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.show_images(x_test, dataset.mean, dataset.std)\n",
    "print(' '.join('%5s' % dataset.get_classes()[l] for l in torch.argmax(y_test, dim=-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of single attention \"\"\"\n",
    "    def __init__(self, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)  # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # sqrt of head size, (B,T,C) @ (B,T,C)^T => (B,T,C) @ (B,C,T) => (B,T,T)\n",
    "        wei = F.softmax(wei, dim=1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x) #(B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) = (B,T,C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat over channel dimension\n",
    "        out = self.proj(out) # projection is a linear transformation of the outcome of the previous multi-head layer\n",
    "        out = self.dropout(out) # dropout\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer of feedforward followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection layer in FFwd\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer of feedforward followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # projection layer in FFwd\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer Block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd : embedding dimension\n",
    "        # n_head : number of heads needed for multi-head self-attention\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0, f'n_embd {n_embd}, n_head: {n_head} must be a divisor'\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, n_embd) # communication\n",
    "        self.ffwd = FeedForward(n_embd) # computation\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # No residual connections\n",
    "        # x = self.sa(x)\n",
    "        # x = self.ffwd(x)\n",
    "        # with residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=96, n_classes=10, n_layers=4, n_heads=6):\n",
    "        super().__init__()\n",
    "        # Every patch sequence begins with a CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        self.patch_embedding_table = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        self.position_embedding_table = nn.Embedding(self.patch_embedding_table.n_patches, embedding_dim=embed_dim)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[ Block(embed_dim, n_heads) for _ in range(n_layers)],\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.vm_head = nn.Linear(embed_dim, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        n_samples, n_chans, n_patch, _ = idx.shape\n",
    "        # print(f\"VisionTransformerModel: n_samples={n_samples}, n_chans={n_chans}, n_patch={n_patch}\")\n",
    "        \n",
    "        patch_emb = self.patch_embedding_table(idx)\n",
    "        # print(f\"VisionTransformerModel: patch_emb shape={patch_emb.shape}\")\n",
    "        n_patches = self.patch_embedding_table.n_patches\n",
    "        pos_emb = self.position_embedding_table(torch.arange(n_patches))\n",
    "        # print(f\"VisionTransformerModel: pos_emb shape={pos_emb.shape}\")\n",
    "        x = patch_emb + pos_emb\n",
    "        # print(f\"VisionTransformerModel: x (after patch+pos) shape={x.shape}\")\n",
    "\n",
    "        # Prepend the cls_token\n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1)\n",
    "        # print(f\"VisionTransformerModel: cls_token shape={cls_token.shape}\")\n",
    "        x =  torch.cat((cls_token, x), dim=1)\n",
    "        # print(f\"VisionTransformerModel: x (after cat cls_token) shape={x.shape}\")\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        # print(f\"VisionTransformerModel: x.blocks shape={x.shape}\")\n",
    "\n",
    "        cls_token_final = x[:, 0]\n",
    "        # print(f\"VisionTransformerModel: cls_token_final shape={cls_token_final.shape}\")\n",
    "\n",
    "        logits = self.softmax(self.vm_head(cls_token_final))\n",
    "        \n",
    "        # print(f\"VisionTransformerModel: logits shape={logits.shape}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # B, T, C = logits.shape\n",
    "            # print(f\"VisionTransformerModel: B T C shape={B} {T} {C}\")\n",
    "            # logits = logits.view(B*T, C)\n",
    "            # targets = targets.view(B*T)\n",
    "            # print(f\"VisionTransformerModel: Logits = {logits.shape}\\n{logits}\\n\")\n",
    "            # print(f\"VisionTransformerModel: Targets = {targets.shape}\\n{targets}\\n\")\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(dataset, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-3\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "from tqdm import tqdm\n",
    "\n",
    "for iter in tqdm(range(max_iters)):\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(dataset, 'train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt, yt = get_batch(dataset, 'test', 100)\n",
    "xy, yt = xt.to(device), yt.to(device)\n",
    "k = 3\n",
    "for _ in range(100):\n",
    "  logits = m(xt)\n",
    "  softmax = nn.Softmax(dim=-1)\n",
    "  probs = softmax(logits)\n",
    "\n",
    "  top_probs, top_ics = probs[0].topk(k)\n",
    "  for i, (ix_, prob_) in enumerate(zip(top_ics, top_probs)):\n",
    "    ix = ix_.item()\n",
    "    prob = prob_.item()\n",
    "    cls = dataset.get_classes()[ix].strip()\n",
    "    print(f\"{i}: {cls:<45} -- {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "981e1182db09a2007bee225472671933ebfc0dce5cb1a519f22c91c47ddc6bdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
