{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPVdtd9oUsYi3Xa6VYEcs4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspiringastro/vit-step-by-step/blob/main/vit_version_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7winnVlZ7Rfw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10DataSet():\n",
        "    def __init__(self, data_dir=\"data/cifar10\", train_val_split=0.8):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = CIFAR10(root=self.data_dir, download=True)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.train_val_split = train_val_split\n",
        "\n",
        "    def  train_dataloader(self, batch_size=32, resize=32, p=0.5, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "                T.RandomResizedCrop(size=resize),\n",
        "                T.RandomHorizontalFlip(p=p),\n",
        "                T.RandomVerticalFlip(p=p),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=True, transform=tf)\n",
        "        num_train = len(ds)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.train_val_split * num_train))\n",
        "        train_sampler = SubsetRandomSampler(indices[split:])\n",
        "        dl = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size, \n",
        "            num_workers=num_workers, \n",
        "            sampler=train_sampler, \n",
        "            drop_last=True\n",
        "            )\n",
        "        return dl\n",
        "    \n",
        "    def  val_dataloader(self, batch_size=32, resize=32, p=0.5, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "                T.RandomResizedCrop(size=resize),\n",
        "                T.RandomHorizontalFlip(p=p),\n",
        "                T.RandomVerticalFlip(p=p),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=True, transform=tf)\n",
        "        num_train = len(ds)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.train_val_split * num_train))\n",
        "        val_sampler = SubsetRandomSampler(indices[:split])\n",
        "        dl = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size, \n",
        "            num_workers=num_workers, \n",
        "            sampler=val_sampler, \n",
        "            drop_last=True\n",
        "            )\n",
        "        return dl\n",
        "\n",
        "    \n",
        "    def test_dataloader(self, batch_size=32, mean=(0.485, 0.456, 0.406) ,std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=False, transform=tf)\n",
        "        dl = DataLoader(ds,batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
        "        return dl\n",
        "    \n",
        "    def get_batch(self, dataloader):\n",
        "        return next(iter(dataloader))\n",
        "    \n",
        "    def get_classes(self):\n",
        "        classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "        return classes\n",
        "\n"
      ],
      "metadata": {
        "id": "jyE-1BRS7XHa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.utils\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "def make_image(img, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    #denormalize\n",
        "    for i in range(3):\n",
        "        img[i] = img[i] * std[i] + mean[i]\n",
        "    npimg = img.numpy()\n",
        "    return np.transpose(npimg, (1, 2, 0))\n",
        "\n",
        "def show_image(imgs, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        for j in range(3):\n",
        "            img[j] = img[j] * std[j] + mean[j]\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "def show_images(imgs, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    grid_imgs = make_grid(imgs)\n",
        "    grid_imgs = make_image(grid_imgs, mean, std)\n",
        "    plt.imshow(grid_imgs)\n",
        "    plt.axis('off')\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "rbich_FF7aRJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "eval_iters = 200\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "DkVgyrqq7cZq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(dataset, split, batch_size=32):\n",
        "    if split == 'train':\n",
        "        downloader = dataset.train_dataloader(batch_size=batch_size)\n",
        "    elif split == 'val':\n",
        "        downloader = dataset.val_dataloader(batch_size=batch_size)\n",
        "    elif split == 'test':\n",
        "        downloader = dataset.test_dataloader(batch_size=batch_size)\n",
        "    else:\n",
        "        raise AttributeError(f'Invalid Split parameter ({split}) provided.')\n",
        "    x, y_label = dataset.get_batch(downloader)\n",
        "    y = F.one_hot(y_label, num_classes=len(dataset.get_classes()))\n",
        "    return x,y.float()\n",
        "\n",
        "dataset = CIFAR10DataSet()\n",
        "\n",
        "x, y = get_batch(dataset, \"train\")\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x_val, y_val = get_batch(dataset, \"val\")\n",
        "print(x_val.shape, y_val.shape)\n",
        "\n",
        "x_test, y_test= get_batch(dataset, \"test\")\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YkG_g0p7jtg",
        "outputId": "173a8c88-88d9-437d-89f5-61d411132100"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch embed layer that takes a 2D image to create embed patches of size P\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=96):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embd = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size, \n",
        "            stride=patch_size\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # print(f\"PatchEmbedding: x.shape: {x.shape}\")\n",
        "        x = self.patch_embd(x)\n",
        "        # print(f\"PatchEmbedding: patch_embd(x).shape: {x.shape}\")\n",
        "        x = x.flatten(2)\n",
        "        # print(f\"PatchEmbedding: flatten(patch_embed(x)).shape: {x.shape}\")\n",
        "        x = x.transpose(1,2)\n",
        "        # print(f\"PatchEmbedding: transpose(flatten(patch_embed(x)),(1,2)).shape: {x.shape}\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "YBxiT17-7jO4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(dataset, split, batch_size=32):\n",
        "    if split == 'train':\n",
        "        downloader = dataset.train_dataloader(batch_size=batch_size)\n",
        "    elif split == 'val':\n",
        "        downloader = dataset.val_dataloader(batch_size=batch_size)\n",
        "    elif split == 'test':\n",
        "        downloader = dataset.test_dataloader(batch_size=batch_size)\n",
        "    else:\n",
        "        raise AttributeError(f'Invalid Split parameter ({split}) provided.')\n",
        "    x, y_label = dataset.get_batch(downloader)\n",
        "    y = F.one_hot(y_label, num_classes=len(dataset.get_classes()))\n",
        "    return x,y.float()\n",
        "\n",
        "x, y = get_batch(dataset, \"train\")\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x_val, y_val = get_batch(dataset, \"val\")\n",
        "print(x_val.shape, y_val.shape)\n",
        "\n",
        "x_test, y_test= get_batch(dataset, \"test\")\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66X8t0AM7l7B",
        "outputId": "9cf070e1-8d64-4e97-b4a9-5a9cb3d6ba21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n",
            "torch.Size([32, 3, 32, 32]) torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of single attention \"\"\"\n",
        "    def __init__(self, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # sqrt of head size, (B,T,C) @ (B,T,C)^T => (B,T,C) @ (B,C,T) => (B,T,T)\n",
        "        wei = F.softmax(wei, dim=1) # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) #(B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) = (B,T,C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rueKHPJf730V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat over channel dimension\n",
        "        out = self.proj(out) # projection is a linear transformation of the outcome of the previous multi-head layer\n",
        "        out = self.dropout(out) # dropout\n",
        "        return out"
      ],
      "metadata": {
        "id": "sVLF1dEP8ZwN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer of feedforward followed by non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer in FFwd\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)"
      ],
      "metadata": {
        "id": "PoxA5li98bqP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd : embedding dimension\n",
        "        # n_head : number of heads needed for multi-head self-attention\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, f'n_embd {n_embd}, n_head: {n_head} must be a divisor'\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd) # communication\n",
        "        self.ffwd = FeedForward(n_embd) # computation\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No residual connections\n",
        "        # x = self.sa(x)\n",
        "        # x = self.ffwd(x)\n",
        "        # with residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2W7lbSIf8ddB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=96, n_classes=10, n_layers=4, n_heads=6):\n",
        "        super().__init__()\n",
        "        # Every patch sequence begins with a CLS token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.patch_embedding_table = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        self.position_embedding_table = nn.Embedding(self.patch_embedding_table.n_patches, embedding_dim=embed_dim)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[ Block(embed_dim, n_heads) for _ in range(n_layers)],\n",
        "            nn.LayerNorm(embed_dim),\n",
        "        )\n",
        "        self.vm_head = nn.Linear(embed_dim, n_classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        n_samples, n_chans, n_patch, _ = idx.shape\n",
        "        # print(f\"VisionTransformerModel: n_samples={n_samples}, n_chans={n_chans}, n_patch={n_patch}\")\n",
        "        \n",
        "        patch_emb = self.patch_embedding_table(idx)\n",
        "        # print(f\"VisionTransformerModel: patch_emb shape={patch_emb.shape}\")\n",
        "        n_patches = self.patch_embedding_table.n_patches\n",
        "        pos_emb = self.position_embedding_table(torch.arange(n_patches))\n",
        "        # print(f\"VisionTransformerModel: pos_emb shape={pos_emb.shape}\")\n",
        "        x = patch_emb + pos_emb\n",
        "        # print(f\"VisionTransformerModel: x (after patch+pos) shape={x.shape}\")\n",
        "\n",
        "        # Prepend the cls_token\n",
        "        cls_token = self.cls_token.expand(n_samples, -1, -1)\n",
        "        # print(f\"VisionTransformerModel: cls_token shape={cls_token.shape}\")\n",
        "        x =  torch.cat((cls_token, x), dim=1)\n",
        "        # print(f\"VisionTransformerModel: x (after cat cls_token) shape={x.shape}\")\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        # print(f\"VisionTransformerModel: x.blocks shape={x.shape}\")\n",
        "\n",
        "        cls_token_final = x[:, 0]\n",
        "        # print(f\"VisionTransformerModel: cls_token_final shape={cls_token_final.shape}\")\n",
        "\n",
        "        logits = self.softmax(self.vm_head(cls_token_final))\n",
        "        \n",
        "        # print(f\"VisionTransformerModel: logits shape={logits.shape}\")\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # B, T, C = logits.shape\n",
        "            # print(f\"VisionTransformerModel: B T C shape={B} {T} {C}\")\n",
        "            # logits = logits.view(B*T, C)\n",
        "            # targets = targets.view(B*T)\n",
        "            # print(f\"VisionTransformerModel: Logits = {logits.shape}\\n{logits}\\n\")\n",
        "            # print(f\"VisionTransformerModel: Targets = {targets.shape}\\n{targets}\\n\")\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "    \n"
      ],
      "metadata": {
        "id": "nh53L8el8fe1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformerModel()"
      ],
      "metadata": {
        "id": "2RT1oZGu8kCq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(dataset, split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "OmnQhDPK8kec"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Hy2Zkt8mvv",
        "outputId": "6e9efa45-3a75-4258-e7c4-6510e91d15dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.458314 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 3e-3\n",
        "max_iters = 1000\n",
        "eval_interval = 100\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "from tqdm import tqdm\n",
        "\n",
        "for iter in tqdm(range(max_iters)):\n",
        "    \n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(dataset, 'train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "z3JtR59h8oV6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}