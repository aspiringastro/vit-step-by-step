{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEZMyP+AsNjfZF9Kb0yTV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aspiringastro/vit-step-by-step/blob/main/vit_version_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7winnVlZ7Rfw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR10DataSet():\n",
        "    def __init__(self, data_dir=\"data/cifar10\", train_val_split=0.8):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = CIFAR10(root=self.data_dir, download=True)\n",
        "        self.mean = (0.485, 0.456, 0.406)\n",
        "        self.std = (0.229, 0.224, 0.225)\n",
        "        self.train_val_split = train_val_split\n",
        "\n",
        "    def  train_dataloader(self, batch_size=32, resize=32, p=0.5, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "                T.RandomResizedCrop(size=resize),\n",
        "                T.RandomHorizontalFlip(p=p),\n",
        "                T.RandomVerticalFlip(p=p),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=True, transform=tf)\n",
        "        num_train = len(ds)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.train_val_split * num_train))\n",
        "        train_sampler = SubsetRandomSampler(indices[split:])\n",
        "        dl = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size, \n",
        "            num_workers=num_workers, \n",
        "            sampler=train_sampler, \n",
        "            drop_last=True\n",
        "            )\n",
        "        return dl\n",
        "    \n",
        "    def  val_dataloader(self, batch_size=32, resize=32, p=0.5, mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225), num_workers=4):\n",
        "        tf = T.Compose([\n",
        "                T.RandomResizedCrop(size=resize),\n",
        "                T.RandomHorizontalFlip(p=p),\n",
        "                T.RandomVerticalFlip(p=p),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=True, transform=tf)\n",
        "        num_train = len(ds)\n",
        "        indices = list(range(num_train))\n",
        "        split = int(np.floor(self.train_val_split * num_train))\n",
        "        val_sampler = SubsetRandomSampler(indices[:split])\n",
        "        dl = DataLoader(\n",
        "            ds,\n",
        "            batch_size=batch_size, \n",
        "            num_workers=num_workers, \n",
        "            sampler=val_sampler, \n",
        "            drop_last=True\n",
        "            )\n",
        "        return dl\n",
        "\n",
        "    \n",
        "    def test_dataloader(self, batch_size=32, mean=(0.485, 0.456, 0.406) ,std=(0.229, 0.224, 0.225), num_workers=2):\n",
        "        tf = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean, std),\n",
        "            ]\n",
        "        )\n",
        "        ds = CIFAR10(root=self.data_dir, train=False, transform=tf)\n",
        "        dl = DataLoader(ds,batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
        "        return dl\n",
        "    \n",
        "    def get_next(self, dl):\n",
        "        return next(iter(dl))\n",
        "    \n",
        "    def get_classes(self):\n",
        "        classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "        return classes\n",
        "\n"
      ],
      "metadata": {
        "id": "jyE-1BRS7XHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.utils\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "\n",
        "def make_image(img, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    #denormalize\n",
        "    for i in range(3):\n",
        "        img[i] = img[i] * std[i] + mean[i]\n",
        "    npimg = img.numpy()\n",
        "    return np.transpose(npimg, (1, 2, 0))\n",
        "\n",
        "def show_image(imgs, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        for j in range(3):\n",
        "            img[j] = img[j] * std[j] + mean[j]\n",
        "        img = img.detach()\n",
        "        img = F.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "def show_images(imgs, mean=(0., 0., 0.), std=(1., 1., 1.)):\n",
        "    grid_imgs = make_grid(imgs)\n",
        "    grid_imgs = make_image(grid_imgs, mean, std)\n",
        "    plt.imshow(grid_imgs)\n",
        "    plt.axis('off')\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "rbich_FF7aRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "eval_iters = 50\n",
        "dropout = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 1e-2\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "n_embd = 384\n",
        "img_size=128"
      ],
      "metadata": {
        "id": "DkVgyrqq7cZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CIFAR10DataSet()\n",
        "downloader = dataset.train_dataloader()"
      ],
      "metadata": {
        "id": "4KvJ8QfxADTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(dataset, split, batch_size=32, resize=img_size):\n",
        "    if split == 'train':\n",
        "        downloader = dataset.train_dataloader(batch_size=batch_size, resize=resize)\n",
        "    elif split == 'val':\n",
        "        downloader = dataset.val_dataloader(batch_size=batch_size, resize=resize)\n",
        "    elif split == 'test':\n",
        "        downloader = dataset.test_dataloader(batch_size=batch_size, resize=resize)\n",
        "    else:\n",
        "        raise AttributeError(f'Invalid Split parameter ({split}) provided.')\n",
        "    x, y_label = dataset.get_next(downloader)\n",
        "    y = F.one_hot(y_label, num_classes=len(dataset.get_classes()))\n",
        "    x, y = x.to(device), y.float().to(device)\n",
        "    return x,y\n",
        "\n",
        "dataset = CIFAR10DataSet()\n",
        "\n",
        "x, y = get_batch(dataset, \"train\")\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "x_val, y_val = get_batch(dataset, \"val\")\n",
        "print(x_val.shape, y_val.shape)\n",
        "\n",
        "x_test, y_test= get_batch(dataset, \"test\")\n",
        "print(x_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "1YkG_g0p7jtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch embed layer that takes a 2D image to create embed patches of size P\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=n_embd):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        self.patch_embd = nn.Conv2d(\n",
        "            in_chans,\n",
        "            embed_dim,\n",
        "            kernel_size=patch_size, \n",
        "            stride=patch_size,\n",
        "            device=device,\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # print(f\"PatchEmbedding: x.shape: {x.shape}\")\n",
        "        x = self.patch_embd(x)\n",
        "        # print(f\"PatchEmbedding: patch_embd(x).shape: {x.shape}\")\n",
        "        x = x.flatten(2)\n",
        "        # print(f\"PatchEmbedding: flatten(patch_embed(x)).shape: {x.shape}\")\n",
        "        x = x.transpose(1,2)\n",
        "        # print(f\"PatchEmbedding: transpose(flatten(patch_embed(x)),(1,2)).shape: {x.shape}\")\n",
        "        return x"
      ],
      "metadata": {
        "id": "YBxiT17-7jO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of single attention \"\"\"\n",
        "    def __init__(self, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)  # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # sqrt of head size, (B,T,C) @ (B,T,C)^T => (B,T,C) @ (B,C,T) => (B,T,T)\n",
        "        wei = F.softmax(wei, dim=1) # (B,T,T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) #(B,T,C)\n",
        "        out = wei @ v # (B,T,T) @ (B,T,C) = (B,T,C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rueKHPJf730V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, n_embd):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, n_embd) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat over channel dimension\n",
        "        out = self.proj(out) # projection is a linear transformation of the outcome of the previous multi-head layer\n",
        "        out = self.dropout(out) # dropout\n",
        "        return out"
      ],
      "metadata": {
        "id": "sVLF1dEP8ZwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer of feedforward followed by non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer in FFwd\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)"
      ],
      "metadata": {
        "id": "PoxA5li98bqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd : embedding dimension\n",
        "        # n_head : number of heads needed for multi-head self-attention\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, f'n_embd {n_embd}, n_head: {n_head} must be a divisor'\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, n_embd) # communication\n",
        "        self.ffwd = FeedForward(n_embd) # computation\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No residual connections\n",
        "        # x = self.sa(x)\n",
        "        # x = self.ffwd(x)\n",
        "        # with residual connection\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2W7lbSIf8ddB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size=img_size, patch_size=4, in_chans=3, embed_dim=n_embd, n_classes=10, n_layers=4, n_heads=6):\n",
        "        super().__init__()\n",
        "        # Every patch sequence begins with a CLS token\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "        self.patch_embedding_table = PatchEmbedding(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        self.position_embedding_table = nn.Embedding(self.patch_embedding_table.n_patches, embedding_dim=embed_dim)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[ Block(embed_dim, n_heads) for _ in range(n_layers)],\n",
        "            nn.LayerNorm(embed_dim),\n",
        "        )\n",
        "        self.vm_head = nn.Linear(embed_dim, n_classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        n_samples, n_chans, n_patch, _ = idx.shape\n",
        "        # print(f\"VisionTransformerModel: n_samples={n_samples}, n_chans={n_chans}, n_patch={n_patch}\")\n",
        "        \n",
        "        patch_emb = self.patch_embedding_table(idx)\n",
        "        # print(f\"VisionTransformerModel: patch_emb shape={patch_emb.shape}\")\n",
        "        n_patches = self.patch_embedding_table.n_patches\n",
        "        pos_emb = self.position_embedding_table(torch.arange(n_patches, device=device))\n",
        "        # print(f\"VisionTransformerModel: pos_emb shape={pos_emb.shape}\")\n",
        "        x = patch_emb + pos_emb\n",
        "        # print(f\"VisionTransformerModel: x (after patch+pos) shape={x.shape}\")\n",
        "\n",
        "        # Prepend the cls_token\n",
        "        cls_token = self.cls_token.expand(n_samples, -1, -1)\n",
        "        # print(f\"VisionTransformerModel: cls_token shape={cls_token.shape}\")\n",
        "        x =  torch.cat((cls_token, x), dim=1)\n",
        "        # print(f\"VisionTransformerModel: x (after cat cls_token) shape={x.shape}\")\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        # print(f\"VisionTransformerModel: x.blocks shape={x.shape}\")\n",
        "\n",
        "        cls_token_final = x[:, 0]\n",
        "        # print(f\"VisionTransformerModel: cls_token_final shape={cls_token_final.shape}\")\n",
        "\n",
        "        logits = self.softmax(self.vm_head(cls_token_final))\n",
        "        \n",
        "        # print(f\"VisionTransformerModel: logits shape={logits.shape}\")\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # B, T, C = logits.shape\n",
        "            # print(f\"VisionTransformerModel: B T C shape={B} {T} {C}\")\n",
        "            # logits = logits.view(B*T, C)\n",
        "            # targets = targets.view(B*T)\n",
        "            # print(f\"VisionTransformerModel: Logits = {logits.shape}\\n{logits}\\n\")\n",
        "            # print(f\"VisionTransformerModel: Targets = {targets.shape}\\n{targets}\\n\")\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "    \n"
      ],
      "metadata": {
        "id": "nh53L8el8fe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformerModel()\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "2RT1oZGu8kCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(dataset, split)\n",
        "            logits, loss = m(X.to(device), Y.to(device))\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "OmnQhDPK8kec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
      ],
      "metadata": {
        "id": "Q3Hy2Zkt8mvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "from tqdm import tqdm\n",
        "\n",
        "for it in tqdm(range(max_iters)):\n",
        "    \n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if it % eval_interval == 0 or it == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"\\tstep {it}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(dataset, 'train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb.to(device), yb.to(device))\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "z3JtR59h8oV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(m, \"vit_base_attempt_1.pth\")"
      ],
      "metadata": {
        "id": "v-FM1nu5JPtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = 100\n",
        "xt, yt = get_batch(dataset, 'test', n_samples)\n",
        "xy, yt = xt.to(device), yt.to(device)\n",
        "k = 3\n",
        "labels = dataset.get_classes()\n",
        "logits = model(xt.to(device))\n",
        "print(logits[0].shape)\n",
        "softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "for j in range(n_samples):\n",
        "  target = labels[torch.argmax(yt[j])]\n",
        "  probs = softmax(logits[0][j])\n",
        "  top_probs, top_ics = probs.topk(k)\n",
        "\n",
        "\n",
        "  for i, (ix_, prob_) in enumerate(zip(top_ics, top_probs)):\n",
        "    ix = ix_.item()\n",
        "    prob = prob_.item()\n",
        "    cls = labels[ix].strip()\n",
        "    print(f\"{i}: {cls:<45} -- {prob*100.0:2.1f}% -- {target}\")"
      ],
      "metadata": {
        "id": "VOB10tl8fBEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Uc-2SSwiyum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}